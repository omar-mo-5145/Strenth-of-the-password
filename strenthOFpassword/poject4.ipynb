{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:31:14.682038Z","iopub.execute_input":"2025-09-14T07:31:14.682719Z","iopub.status.idle":"2025-09-14T07:31:14.932774Z","shell.execute_reply.started":"2025-09-14T07:31:14.682693Z","shell.execute_reply":"2025-09-14T07:31:14.932029Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:31:18.088314Z","iopub.execute_input":"2025-09-14T07:31:18.088935Z","iopub.status.idle":"2025-09-14T07:31:18.569212Z","shell.execute_reply.started":"2025-09-14T07:31:18.088912Z","shell.execute_reply":"2025-09-14T07:31:18.568422Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e5fbbd990a489e8d1fe0c6436b3d16"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip -q install \"transformers>=4.43\" \"accelerate>=0.33\" \"bitsandbytes>=0.43\" \"autoawq>=0.2.7\" torch --extra-index-url https://download.pytorch.org/whl/cu121","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:33:04.576171Z","iopub.execute_input":"2025-09-14T07:33:04.576790Z","iopub.status.idle":"2025-09-14T07:34:33.832349Z","shell.execute_reply.started":"2025-09-14T07:33:04.576765Z","shell.execute_reply":"2025-09-14T07:34:33.831437Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch, os, shutil, pathlib\n\nbase_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntok = AutoTokenizer.from_pretrained(base_id, use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:35:42.634576Z","iopub.execute_input":"2025-09-14T07:35:42.635228Z","iopub.status.idle":"2025-09-14T07:35:53.326627Z","shell.execute_reply.started":"2025-09-14T07:35:42.635197Z","shell.execute_reply":"2025-09-14T07:35:53.325944Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b2034c23de4fe597b9d1bf4176d64a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b97d699abc5f47b9988381da242b99ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"431afd525b7f4db783525bcc7089f5ad"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\nmodel = AutoModelForCausalLM.from_pretrained(base_id, quantization_config=bnb_cfg, device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:36:16.661297Z","iopub.execute_input":"2025-09-14T07:36:16.661744Z","iopub.status.idle":"2025-09-14T07:41:32.169593Z","shell.execute_reply.started":"2025-09-14T07:36:16.661721Z","shell.execute_reply":"2025-09-14T07:41:32.168922Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3ce48b36434c8eadec4363cfe1321e"}},"metadata":{}},{"name":"stderr","text":"2025-09-14 07:36:25.027873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757835385.366370      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757835385.458738      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/29.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b24e352bfa049ba9d5307454fbfd59b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f1b1d06e7140cabc92568326e7091b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960966dfbe9a47f19f270f3f78193b19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b3423f5305c462bb91256296e5bf9e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a48a707c8b43839ed147acd6084428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7aa1eaddf3c407b84e07dcf207ca8c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6dcc7219b7f4eb7b71257933a69588f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd1e2d6f7e8d43029ba6237a3c188462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4836e3f35f914c489cfc8d5bfbef245d"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"save_dir = \"mistral_nemo_instruct_2407_4bit_bnb\"\ntry:\n    model.save_pretrained(save_dir, safe_serialization=True)\n    tok.save_pretrained(save_dir)\n    print(\"Saved quantized model to:\", save_dir)\nexcept Exception as e:\n    print(\"Save 4-bit not supported in this env:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:41:36.609625Z","iopub.execute_input":"2025-09-14T07:41:36.610696Z","iopub.status.idle":"2025-09-14T07:42:01.338632Z","shell.execute_reply.started":"2025-09-14T07:41:36.610670Z","shell.execute_reply":"2025-09-14T07:42:01.337663Z"}},"outputs":[{"name":"stdout","text":"Saved quantized model to: mistral_nemo_instruct_2407_4bit_bnb\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Ù…ÙƒØ§Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù„ÙŠ Ø­ÙØ¸ØªÙ‡Ø§\nsave_dir = \"mistral_nemo_instruct_2407_4bit_bnb\"\n\n# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ tokenizer Ùˆ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„\ntok = AutoTokenizer.from_pretrained(save_dir, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(save_dir, device_map=\"auto\")\n\n# Function Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ\ndef generate_text(prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        do_sample=True\n    )\n    return tok.decode(outputs[0], skip_special_tokens=True)\n\n# Ù†Ø¬Ø±Ø¨ prompt Ø¨Ø³ÙŠØ·\nprompt = \"who is messi ?\"\nprint(generate_text(prompt))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:42:53.879060Z","iopub.execute_input":"2025-09-14T07:42:53.879605Z","iopub.status.idle":"2025-09-14T07:43:10.545276Z","shell.execute_reply.started":"2025-09-14T07:42:53.879580Z","shell.execute_reply":"2025-09-14T07:43:10.544645Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d5e8751065f41ab9e21ad9741c05e4a"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"who is messi ? why is he called messi ?\n- Lionel Messi is a professional footballer who plays for Paris Saint-Germain and the Argentina national team. He is widely regarded as one of the greatest players in the history of the sport.\n\nHe is called \"Messi\" because that is his surname. In Spanish, \"Messi\" is pronounced as \"MEH-see\". His full name is Lionel AndrÃ©s Messi Cuccittini.\n\nHis nickname is \"La Pulga,\" which translates to \"The Flea\"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from langchain.llms.base import LLM\nfrom typing import Any\n\nclass CustomHFLLM(LLM):\n    def _call(self, prompt: str, stop: Any = None) -> str:\n        return generate_text(prompt)\n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom_huggingface\"\n\nllm = CustomHFLLM()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:43:52.285787Z","iopub.execute_input":"2025-09-14T07:43:52.286316Z","iopub.status.idle":"2025-09-14T07:43:54.954987Z","shell.execute_reply.started":"2025-09-14T07:43:52.286279Z","shell.execute_reply":"2025-09-14T07:43:54.954262Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import requests\nimport hashlib\n\nclass BreachChecker:\n    def __init__(self):\n        self.api_url = \"https://api.pwnedpasswords.com/range/\"\n    \n    def check_password(self, password: str) -> str:\n        sha1_hash = hashlib.sha1(password.encode('utf-8')).hexdigest().upper()\n        prefix = sha1_hash[:5]\n        suffix = sha1_hash[5:]\n        \n        response = requests.get(self.api_url + prefix)\n        if response.status_code != 200:\n            return \"Error contacting HIBP API\"\n        \n        hashes = (line.split(':') for line in response.text.splitlines())\n        for h, count in hashes:\n            if h == suffix:\n                return f\"This password has been seen {count} times before in data breaches!\"\n        \n        return \"Your password was NOT found in known breaches. ğŸ‘\"\n\nclass EmailBreachChecker:\n    def __init__(self, api_key: str):\n        self.api_url = \"https://haveibeenpwned.com/api/v3/breachedaccount/\"\n        self.headers = {\n            \"hibp-api-key\": api_key,\n            \"user-agent\": \"PasswordSecurityProject\"\n        }\n    \n    def check_email(self, email: str):\n        url = self.api_url + email\n        response = requests.get(url, headers=self.headers)\n        \n        if response.status_code == 200:\n            breaches = response.json()\n            result = []\n            for breach in breaches:\n                result.append(f\"Leaked from {breach['Name']} in {breach['BreachDate']}\")\n            return result\n        elif response.status_code == 404:\n            return [\"âœ… Good news â€” this email was NOT found in any breaches.\"]\n        else:\n            return [f\"âš ï¸ Error: {response.status_code} - {response.text}\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:43:58.727312Z","iopub.execute_input":"2025-09-14T07:43:58.727924Z","iopub.status.idle":"2025-09-14T07:43:58.735083Z","shell.execute_reply.started":"2025-09-14T07:43:58.727901Z","shell.execute_reply":"2025-09-14T07:43:58.734393Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# Chain 1: Evaluate password strength\nstrength_prompt = PromptTemplate(\n    input_variables=[\"password\"],\n    template=\"\"\"\nyour password {password} strength evaluator is week or meduim or strong. \nAnswer with only one word and nothing else.\"\"\")\n\nstrength_chain = LLMChain(llm=llm, prompt=strength_prompt)\n\n# Chain 2: Explain why\nexplain_prompt = PromptTemplate(\n    input_variables=[\"password\", \"strength\"],\n    template=\"\"\"\nYou are a cybersecurity expert. \nExplain clearly and logically why the password \"{password}\" is considered {strength}.\nGive a short but professional explanation.\n\"\"\"\n)\nexplain_chain = LLMChain(llm=llm, prompt=explain_prompt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:11:15.208561Z","iopub.execute_input":"2025-09-14T08:11:15.208905Z","iopub.status.idle":"2025-09-14T08:11:15.213946Z","shell.execute_reply.started":"2025-09-14T08:11:15.208876Z","shell.execute_reply":"2025-09-14T08:11:15.213338Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def check_password_pipeline(password: str):\n    print(f\"\\nPassword: {password}\")\n    \n    # Step 1: strength\n    strength = strength_chain.run(password).strip()\n    print(f\"\\nPassword Strength:\\n{strength}\")\n    \n    # Step 2: explanation\n    explanation = explain_chain.run({\"password\": password, \"strength\": strength}).strip()\n    print(f\"\\nExplanation:\\n{explanation}\")\n    \n    # Ø±Ø¬Ù‘Ø¹ dict Ø¹Ø´Ø§Ù† ØªÙ‚Ø¯Ø± ØªØ³ØªØ®Ø¯Ù…Ù‡ Ø¨Ø±Ø§\n    return {\n        \"strength\": strength,\n        \"explanation\": explanation\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:11:18.711316Z","iopub.execute_input":"2025-09-14T08:11:18.711550Z","iopub.status.idle":"2025-09-14T08:11:18.716013Z","shell.execute_reply.started":"2025-09-14T08:11:18.711534Z","shell.execute_reply":"2025-09-14T08:11:18.715217Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"user_password = input(\"Enter a password: \")\n\nresult = check_password_pipeline(user_password)\n\nprint(\"Strength:\", result[\"strength\"], \"\\n\")\nprint(\"Explanation:\", result[\"explanation\"], \"\\n\")\nprint(\"\\nBreach Info:\", BreachChecker().check_password(user_password))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:11:23.007565Z","iopub.execute_input":"2025-09-14T08:11:23.008363Z","iopub.status.idle":"2025-09-14T08:11:29.005959Z","shell.execute_reply.started":"2025-09-14T08:11:23.008333Z","shell.execute_reply":"2025-09-14T08:11:29.005041Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a password:  hello\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nPassword: hello\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nPassword Strength:\nyour password hello strength evaluator is week or meduim or strong. \nAnswer with only one word and nothing else.\n\nExplanation:\nYou are a cybersecurity expert. \nExplain clearly and logically why the password \"hello\" is considered your password hello strength evaluator is week or meduim or strong. \nAnswer with only one word and nothing else..\nGive a short but professional explanation.\nWeak\nStrength: your password hello strength evaluator is week or meduim or strong. \nAnswer with only one word and nothing else. \n\nExplanation: You are a cybersecurity expert. \nExplain clearly and logically why the password \"hello\" is considered your password hello strength evaluator is week or meduim or strong. \nAnswer with only one word and nothing else..\nGive a short but professional explanation.\nWeak \n\n\nBreach Info: This password has been seen 403640 times before in data breaches!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!pip install fastapi uvicorn pyngrok nest_asyncio -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:39:59.370231Z","iopub.execute_input":"2025-09-14T08:39:59.370566Z","iopub.status.idle":"2025-09-14T08:40:04.557433Z","shell.execute_reply.started":"2025-09-14T08:39:59.370541Z","shell.execute_reply":"2025-09-14T08:40:04.556652Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from pyngrok import ngrok, conf\n\nNGROK_TOKEN = \"27k71fogGnL8noLjXKUe9aluAOw_3b4emprEQGwpDtvChuDSQ\"\nconf.get_default().auth_token = NGROK_TOKEN\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:40:08.454757Z","iopub.execute_input":"2025-09-14T08:40:08.455048Z","iopub.status.idle":"2025-09-14T08:40:08.458953Z","shell.execute_reply.started":"2025-09-14T08:40:08.455028Z","shell.execute_reply":"2025-09-14T08:40:08.458249Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from fastapi import FastAPI, Request, HTTPException\nfrom pyngrok import ngrok\nimport nest_asyncio, uvicorn\n\nAPI_KEY = \"secret123\"\napp = FastAPI()\n\n@app.post(\"/check-password\")\nasync def check_password(req: Request):\n    if req.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n    \n    data = await req.json()\n    password = data.get(\"password\", \"\")\n    if not password:\n        raise HTTPException(status_code=400, detail=\"Password is required\")\n    \n    # Ù‡Ù†Ø§ ØªØ³ØªØ®Ø¯Ù… pipeline Ø¨ØªØ§Ø¹Ùƒ Ù…Ù† Cell 11\n    result = check_password_pipeline(password)\n    breach_info = BreachChecker().check_password(password)\n\n    return {\n        \"strength\": result[\"strength\"],\n        \"explanation\": result[\"explanation\"],\n        \"breach\": breach_info\n    }\n\n# Run server\nimport threading, time\nimport nest_asyncio\nnest_asyncio.apply()\n\npublic_url = ngrok.connect(8000)\nprint(\"Public URL:\", public_url)\nthreading.Thread(target=uvicorn.run, args=(app,), kwargs={\"host\":\"0.0.0.0\", \"port\":8000}).start()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:40:16.007166Z","iopub.execute_input":"2025-09-14T08:40:16.008021Z","iopub.status.idle":"2025-09-14T08:40:17.382261Z","shell.execute_reply.started":"2025-09-14T08:40:16.007969Z","shell.execute_reply":"2025-09-14T08:40:17.381175Z"}},"outputs":[{"name":"stdout","text":"Public URL: NgrokTunnel: \"https://87b3b16b19a5.ngrok-free.app\" -> \"http://localhost:8000\"           \n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [36]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"}],"execution_count":33}]}